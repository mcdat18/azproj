{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "from azure.mgmt.datafactory.models import *\n",
    "from msrest.authentication import BasicTokenAuthentication\n",
    "from azure.core.pipeline.policies import BearerTokenCredentialPolicy\n",
    "from azure.core.pipeline import PipelineRequest, PipelineContext\n",
    "from azure.core.pipeline.transport import HttpRequest\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create unformation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_item(group):\n",
    "    \"\"\"Print an Azure object instance.\"\"\"\n",
    "    print(\"\\tName: {}\".format(group.name))\n",
    "    print(\"\\tId: {}\".format(group.id))\n",
    "    if hasattr(group, 'location'):\n",
    "        print(\"\\tLocation: {}\".format(group.location))\n",
    "    if hasattr(group, 'tags'):\n",
    "        print(\"\\tTags: {}\".format(group.tags))\n",
    "    if hasattr(group, 'properties'):\n",
    "        print_properties(group.properties)\n",
    "\n",
    "\n",
    "def print_properties(props):\n",
    "    \"\"\"Print a ResourceGroup properties instance.\"\"\"\n",
    "    if props and hasattr(props, 'provisioning_state') and props.provisioning_state:\n",
    "        print(\"\\tProperties:\")\n",
    "        print(\"\\t\\tProvisioning State: {}\".format(props.provisioning_state))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def print_activity_run_details(activity_run):\n",
    "    \"\"\"Print activity run details.\"\"\"\n",
    "    print(\"\\n\\tActivity run details\\n\")\n",
    "    print(\"\\tActivity run status: {}\".format(activity_run.status))\n",
    "    if activity_run.status == 'Succeeded':\n",
    "        print(\"\\tNumber of bytes read: {}\".format(activity_run.output['dataRead']))\n",
    "        print(\"\\tNumber of bytes written: {}\".format(activity_run.output['dataWritten']))\n",
    "        print(\"\\tCopy duration: {}\".format(activity_run.output['copyDuration']))\n",
    "    else:\n",
    "        print(\"\\tErrors: {}\".format(activity_run.error['message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add credential class whapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CredentialWrapper(BasicTokenAuthentication):\n",
    "    def __init__(self, credential=None, resource_id=\"https://management.azure.com/.default\", **kwargs):\n",
    "        \"\"\"Wrap any azure-identity credential to work with SDK that needs azure.common.credentials/msrestazure.\n",
    "        Default resource is ARM (syntax of endpoint v2)\n",
    "        :param credential: Any azure-identity credential (DefaultAzureCredential by default)\n",
    "        :param str resource_id: The scope to use to get the token (default ARM)\n",
    "        \"\"\"\n",
    "        super(CredentialWrapper, self).__init__(None)\n",
    "        if credential is None:\n",
    "            credential = DefaultAzureCredential()\n",
    "        self._policy = BearerTokenCredentialPolicy(credential, resource_id, **kwargs)\n",
    "\n",
    "    def _make_request(self):\n",
    "        return PipelineRequest(\n",
    "            HttpRequest(\n",
    "                \"CredentialWrapper\",\n",
    "                \"https://fakeurl\"\n",
    "            ),\n",
    "            PipelineContext(None)\n",
    "        )\n",
    "\n",
    "    def set_token(self):\n",
    "        \"\"\"Ask the azure-core BearerTokenCredentialPolicy policy to get a token.\n",
    "        Using the policy gives us for free the caching system of azure-core.\n",
    "        We could make this code simpler by using private method, but by definition\n",
    "        I can't assure they will be there forever, so mocking a fake call to the policy\n",
    "        to extract the token, using 100% public API.\"\"\"\n",
    "        request = self._make_request()\n",
    "        self._policy.on_request(request)\n",
    "        # Read Authorization, and get the second part after Bearer\n",
    "        token = request.http_request.headers[\"Authorization\"].split(\" \", 1)[1]\n",
    "        self.token = {\"access_token\": token}\n",
    "\n",
    "    def signed_session(self, session=None):\n",
    "        self.set_token()\n",
    "        return super(CredentialWrapper, self).signed_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Authentificate Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Azure subscription ID\n",
    "subscription_id = 'd388216e-67b8-41b3-9832-d6511a1a70be'\n",
    "\n",
    "rg_name = 'ADFCookbook'\n",
    "df_name = 'ADFCookbook-From-Python' \n",
    "\n",
    "credential = ClientSecretCredential(\n",
    "        tenant_id='enter tenant id',\n",
    "        client_id='enterclient id',\n",
    "        client_secret='enter client secret id'\n",
    ")\n",
    "credentials = CredentialWrapper(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tName: ADFCookbook-From-Python\n",
      "\tId: /subscriptions/d388216e-67b8-41b3-9832-d6511a1a70be/resourceGroups/adfcookbook/providers/Microsoft.DataFactory/factories/adfcookbook-from-python\n",
      "\tLocation: eastus\n",
      "\tTags: {}\n"
     ]
    }
   ],
   "source": [
    "adf_client = DataFactoryManagementClient(credentials, subscription_id)\n",
    "df_resource = Factory(location='eastus')\n",
    "df = adf_client.factories.create_or_update(rg_name, df_name, df_resource)\n",
    "print_item(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Created a linked service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tName: ADFCookbookLinkedServicePython\n",
      "\tId: /subscriptions/d388216e-67b8-41b3-9832-d6511a1a70be/resourceGroups/ADFCookbook/providers/Microsoft.DataFactory/factories/ADFCookbook-From-Python/linkedservices/ADFCookbookLinkedServicePython\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Azure Storage linked service\n",
    "ls_name = 'ADFCookbookLinkedServicePython'\n",
    "\n",
    "# IMPORTANT: specify the name and key of your Azure Storage account.\n",
    "storage_string = SecureString(value='DefaultEndpointsProtocol=https;AccountName=adfcookbookstorage;AccountKey=enter your account key')\n",
    "\n",
    "ls_azure_storage = AzureStorageLinkedService(connection_string=storage_string)\n",
    "ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)\n",
    "print_item(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tName: ADFCookbookDS-Input-Python\n",
      "\tId: /subscriptions/d388216e-67b8-41b3-9832-d6511a1a70be/resourceGroups/ADFCookbook/providers/Microsoft.DataFactory/factories/ADFCookbook-From-Python/datasets/ADFCookbookDS-Input-Python\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'ADFCookbookDS-Input-Python'\n",
    "ds_ls = LinkedServiceReference(reference_name=ls_name)\n",
    "blob_path= 'adfcookbook/input'\n",
    "blob_filename = 'SalesOrders.txt'\n",
    "ds_azure_blob= AzureBlobDataset(linked_service_name=ds_ls, folder_path=blob_path, file_name = blob_filename)\n",
    "ds = adf_client.datasets.create_or_update(rg_name, df_name, ds_name, ds_azure_blob)\n",
    "print_item(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tName: ADFCookbookDS-Output-Python\n",
      "\tId: /subscriptions/d388216e-67b8-41b3-9832-d6511a1a70be/resourceGroups/ADFCookbook/providers/Microsoft.DataFactory/factories/ADFCookbook-From-Python/datasets/ADFCookbookDS-Output-Python\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dsOut_name = 'ADFCookbookDS-Output-Python'\n",
    "output_blobpath = 'adfcookbook/output'\n",
    "dsOut_azure_blob = AzureBlobDataset(linked_service_name=ds_ls, folder_path=output_blobpath)\n",
    "dsOut = adf_client.datasets.create_or_update(rg_name, df_name, dsOut_name, dsOut_azure_blob)\n",
    "print_item(dsOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tName: ADFCookbookCopyDataPipeline\n",
      "\tId: /subscriptions/d388216e-67b8-41b3-9832-d6511a1a70be/resourceGroups/ADFCookbook/providers/Microsoft.DataFactory/factories/ADFCookbook-From-Python/pipelines/ADFCookbookCopyDataPipeline\n"
     ]
    }
   ],
   "source": [
    "# Create a copy activity\n",
    "act_name = 'ADFCookbookCopyData'\n",
    "blob_source = BlobSource()\n",
    "blob_sink = BlobSink()\n",
    "dsin_ref = DatasetReference(reference_name=ds_name)\n",
    "dsOut_ref = DatasetReference(reference_name=dsOut_name)\n",
    "copy_activity = CopyActivity(name=act_name,inputs=[dsin_ref], outputs=[dsOut_ref], source=blob_source, sink=blob_sink)\n",
    "\n",
    "#Create a pipeline with the copy activity\n",
    "p_name = 'ADFCookbookCopyDataPipeline'\n",
    "params_for_pipeline = {}\n",
    "p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)\n",
    "p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)\n",
    "print_item(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_response = adf_client.pipelines.create_run(rg_name, df_name, p_name, parameters={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monitor a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tPipeline run status: Succeeded\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = adf_client.pipeline_runs.get(rg_name, df_name, run_response.run_id)\n",
    "print(\"\\n\\tPipeline run status: {}\".format(pipeline_run.status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
